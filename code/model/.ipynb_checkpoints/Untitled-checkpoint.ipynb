{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_token: [101, 791, 1921, 1921, 3706, 4696, 100, 511, 102]\n",
      "sent_token_padding: [[ 101  791 1921 1921 3706 4696  100  511  102    0]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Tokenizer and Bert Model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "embedding = AutoModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "sent = '今天天氣真 Good。'\n",
    "sent_token = tokenizer.encode(sent)\n",
    "sent_token_padding = pad_sequences([sent_token], maxlen=10, padding='post', dtype='int')\n",
    "masks = [[float(value>0) for value in values] for values in sent_token_padding]\n",
    "\n",
    "print('sent_token:', sent_token)\n",
    "print('sent_token_padding:', sent_token_padding)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Bert-BiGRU-Classifier\n",
    "class BERTmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.embedding = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=768,\n",
    "            hidden_size=768,\n",
    "            dropout=0.3,\n",
    "            num_layers=5,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc_1 = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, tokens, masks=None):\n",
    "        # BERT\n",
    "        embedded, _ = self.embedding(tokens, attention_mask=masks)\n",
    "        cls_vector = embedded[:, 0, :]\n",
    "        cls_vector = cls_vector.view(-1, 1, 768)\n",
    "\n",
    "        # GRU\n",
    "        _, hidden = self.gru(cls_vector)\n",
    "        hidden = hidden[-1]\n",
    "\n",
    "        # Fully-connected layer\n",
    "        outputs = self.fc_1(hidden.squeeze(0))\n",
    "        outputs = self.sigmoid(outputs).view(-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "inputs = torch.tensor(sent_token_padding)\n",
    "masks = torch.tensor(masks)\n",
    "embedded, _ = embedding(inputs, attention_mask=masks)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\"\"\"\"\n",
    "create_mini_batch(samples)吃上面定義的mydataset\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "#collate_fn: 如何將多個樣本的資料連成一個batch丟進 model\n",
    "#截長補短後要限制attention只注意非pad 的部分\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad到該batch下最長的長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 batch size 個訓練樣本的 DataLoader\n",
    "# 利用 'collate_fn' 將 list of samples 合併成一個 mini-batch\n",
    "trainset = MyDataset(\"train\", tokenizer=tokenizer)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch,shuffle=False)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "print(tokens_tensors)\n",
    "print(segments_tensors)\n",
    "print(masks_tensors)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 21128\n",
      "token               index          \n",
      "-------------------------\n",
      "3100                12100\n",
      "##祎                 17916\n",
      "额                    7583\n",
      "##ino                9846\n",
      "旅                    3180\n",
      "##彥                 15560\n",
      "铲                    7211\n",
      "夾                    1933\n",
      "##躪                 19772\n",
      "##殞                 16717\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\" #英文pretrain(不區分大小寫)\n",
    "\n",
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "haha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
