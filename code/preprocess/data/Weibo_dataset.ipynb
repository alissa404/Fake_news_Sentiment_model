{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pip\n",
    "import keras\n",
    "import time\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences # To make vectors the same size. \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, MaxPool1D,concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import plot_model, np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_new = np.load(\"/home/alissa77/WWW2021 copy/code/preprocess/data/Weibo-16/emotions_new/val_(738, 55).npy\")\n",
    "emotions = np.load(\"/home/alissa77/WWW2021 copy/code/preprocess/data/Weibo-16/emotions/val_(738, 55).npy\")\n",
    "cvaw_weibo = np.load(\"/home/alissa77/WWW2021 copy/code/preprocess/data_cvaw_weibo/Weibo-16/emotions/val_(738, 55).npy\")\n",
    "#emotions_new2 = np.load(\"/home/alissa77/WWW2021 copy/code/preprocess/data/Weibo-20/emotions_new/Weibo-20_test_(1272, 275).npy\")"
   ]
  },
  {
   "source": [
    "# prepare sentiment feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "-------------------- [2021-08-11 16:23:39]\tProcessing the dataset: Weibo-16 --------------------\n",
      "\n",
      "Weibo-16_val\n",
      "Weibo-16_test\n",
      "Weibo-16_train\n",
      "\n",
      "\n",
      "-------------------- [2021-08-11 16:23:46]\tProcessing the dataset: Weibo-16-original --------------------\n",
      "\n",
      "Weibo-16-original_val\n",
      "Weibo-16-original_test\n",
      "Weibo-16-original_train\n",
      "\n",
      "\n",
      "-------------------- [2021-08-11 16:23:54]\tProcessing the dataset: Weibo-20 --------------------\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5e038b63cd91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mname1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdic2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'emotions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[1;32m    176\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mupon\u001b[0m \u001b[0munpickling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrather\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mine method + cvaw resource\n",
    "datasets_ch = ['Weibo-16', 'Weibo-16-original',\n",
    "               'Weibo-20', 'Weibo-20-temporal']\n",
    "dataset_dir = '/home/alissa77/WWW2021/code/preprocess/data'\n",
    "dataset_dir_new = '/home/alissa77/fake-news-detect/preprocess_data'\n",
    "save_dir = '../data'\n",
    "tmp = []\n",
    "dic2 = {}\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for dataset in datasets_ch : \n",
    "    print('\\n\\n{} [{}]\\tProcessing the dataset: {} {}\\n'.format(\n",
    "    '-'*20, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()), dataset, '-'*20))\n",
    "    \n",
    "    dic = {}\n",
    "    output_dir = os.path.join(save_dir, dataset)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    emotion_dir = os.path.join(output_dir, 'emotions_new')\n",
    "    if not os.path.exists(emotion_dir):\n",
    "        os.mkdir(emotion_dir)\n",
    "\n",
    "    data_dir_new = os.path.join(dataset_dir_new, dataset)  \n",
    "    for t in ['train', 'val', 'test']:\n",
    "        name1 = data_dir_new.split('/')[5]+\"_\"+t\n",
    "        dic2[name1] = pd.read_pickle(os.path.join(data_dir_new, '{}.pkl'.format(t)))['sentiment_score'] \n",
    "\n",
    "    for t in ['emotions']:\n",
    "        data_dir = os.path.join(dataset_dir, dataset, t)\n",
    "        for f in os.listdir(data_dir):\n",
    "            f = os.path.join(data_dir, f)\n",
    "            name = ''\n",
    "            if 'train_' in f:\n",
    "                name = f.split('/')[7]+\"_train\"\n",
    "                dic[name] = np.load(f)\n",
    "            elif 'val_' in f:\n",
    "                name = f.split('/')[7]+\"_val\"\n",
    "                dic[name] = np.load(f)\n",
    "            elif 'test_' in f:\n",
    "                name = f.split('/')[7]+\"_test\"\n",
    "                dic[name] = np.load(f)\n",
    "        \n",
    "    for arr in dic.keys():\n",
    "        dic[arr][:,37:38] = dic2[arr][:,np.newaxis] # shape(3) -> (3,1) \n",
    "        print(arr)\n",
    "        np.save(os.path.join(emotion_dir, '{}_{}.npy'.format(arr, dic[arr].shape)), dic[arr])"
   ]
  },
  {
   "source": [
    "# change the sentiment feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CVAW]\t There are 5512 words\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-794cae4b869d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mcut_wrds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'我'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'好'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'想'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'睡'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mboson_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_wrds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-794cae4b869d>\u001b[0m in \u001b[0;36mboson_value\u001b[0;34m(cut_words, windows)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlen_cvaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# lyric[i] = a word in CVAW dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mlyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcut_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtemp_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# vep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# 他的方法用在我的cvaw\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# negation value and degree value\n",
    "def get_not_and_how_value(cut_words, i, windows):\n",
    "    not_cnt = 0\n",
    "    how_v = 1\n",
    "    left = 0 if (i - windows) < 0 else (i - windows)\n",
    "    window_text = ' '.join(cut_words[left:i])\n",
    "    for w in negation_words:\n",
    "        if w in window_text:\n",
    "            not_cnt += 1\n",
    "    for w in how_words_dict.keys():\n",
    "        if w in window_text:\n",
    "            how_v *= how_words_dict[w]\n",
    "\n",
    "    return (-1) ** not_cnt, how_v\n",
    "\n",
    "\n",
    "def boson_value(cut_words, windows=2):\n",
    "    def b_edge(value):\n",
    "        if value<0:\n",
    "            return -4\n",
    "        else:\n",
    "            return 4\n",
    "    valence = {}\n",
    "    arousal = {}\n",
    "    with open('/home/alissa77/WWW2021 copy/code/emotion/cvaw4.csv', newline='') as csvfile:\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows:\n",
    "            valence[row['Word']] = (row['Valence_Mean'])\n",
    "            arousal[row['Word']] = (row['Arousal_Mean'])\n",
    "            \n",
    "    valence = {x: round(float(valence[x])-5, 2) for x in valence}\n",
    "    arousal = {x: round(float(arousal[x])-5, 2) for x in arousal}\n",
    "    print('[CVAW]\\t There are {} words'.format(len(arousal)))\n",
    "\n",
    "    for ID in cut_words:\n",
    "        score_v = 0\n",
    "        score_a = 0\n",
    "        score_degree = 0\n",
    "        score_negative = 0\n",
    "        len_cvaw = 1 \n",
    "\n",
    "        for i in range(2, len(cut_words[ID])): # lyric[i] = a word in CVAW dictionary\n",
    "            lyric = cut_words[ID]\n",
    "            temp_v = 0  # vep\n",
    "            temp_a = 0  # aep\n",
    "\n",
    "            if  cut_words[ID] in list(degree):\n",
    "                score_degree += degree[ cut_words[ID]]\n",
    "            if  cut_words[ID] in list(negative):\n",
    "                score_negative += negative[ cut_words[ID]]\n",
    "\n",
    "\n",
    "    value = 0\n",
    "    for i, word in enumerate(cut_words):\n",
    "        if word in arousal:\n",
    "            not_v, how_v = get_not_and_how_value(cut_words, i, windows)\n",
    "            \n",
    "            value += not_v * how_v * arousal[word] #compute right here!!!!!!\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "# def extract_publisher_emotion(content, content_words, emotions_dict):\n",
    "#     text, cut_words = content, content_words\n",
    "#     arr = np.zeros(2)\n",
    "#     arr[:1] = boson_value(cut_words)\n",
    "\n",
    "cut_wrds = ['我','好','想','睡']\n",
    "boson_value(cut_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       0.6\n",
      "1       0.6\n",
      "2       1.2\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "2206    2.0\n",
      "2207   -0.6\n",
      "2208    1.0\n",
      "2209    0.0\n",
      "2210    0.0\n",
      "Name: sentiment_score, Length: 2211, dtype: float64\n",
      "0      3.40\n",
      "1      0.00\n",
      "2      1.60\n",
      "3      5.60\n",
      "4     -0.80\n",
      "       ... \n",
      "733   -0.40\n",
      "734   -0.28\n",
      "735    2.00\n",
      "736    1.44\n",
      "737    0.00\n",
      "Name: sentiment_score, Length: 738, dtype: float64\n",
      "0      0.0\n",
      "1      0.8\n",
      "2      0.0\n",
      "3      4.8\n",
      "4      3.8\n",
      "      ... \n",
      "752    2.8\n",
      "753    0.0\n",
      "754    0.2\n",
      "755   -0.8\n",
      "756    0.8\n",
      "Name: sentiment_score, Length: 757, dtype: float64\n",
      "0       0.60\n",
      "1       0.60\n",
      "2       4.96\n",
      "3       1.20\n",
      "4       0.00\n",
      "        ... \n",
      "2791    2.00\n",
      "2792   -0.60\n",
      "2793    1.00\n",
      "2794    0.00\n",
      "2795    0.00\n",
      "Name: sentiment_score, Length: 2796, dtype: float64\n",
      "0      0.00\n",
      "1      0.00\n",
      "2      0.00\n",
      "3      0.00\n",
      "4      3.40\n",
      "       ... \n",
      "928   -0.40\n",
      "929   -0.28\n",
      "930    2.00\n",
      "931    1.44\n",
      "932    0.00\n",
      "Name: sentiment_score, Length: 933, dtype: float64\n",
      "0      0.0\n",
      "1     -8.0\n",
      "2      0.0\n",
      "3      5.6\n",
      "4      0.0\n",
      "      ... \n",
      "929    2.8\n",
      "930    0.0\n",
      "931    0.2\n",
      "932   -0.8\n",
      "933    0.8\n",
      "Name: sentiment_score, Length: 934, dtype: float64\n",
      "0       1.00\n",
      "1       0.00\n",
      "2      -1.40\n",
      "3       5.20\n",
      "4       1.20\n",
      "        ... \n",
      "3811    1.40\n",
      "3812    0.66\n",
      "3813    0.00\n",
      "3814    0.20\n",
      "3815    0.00\n",
      "Name: sentiment_score, Length: 3816, dtype: float64\n",
      "0       0.0\n",
      "1       0.0\n",
      "2       1.0\n",
      "3      -0.8\n",
      "4       0.0\n",
      "       ... \n",
      "1267    0.0\n",
      "1268    0.2\n",
      "1269    0.0\n",
      "1270    0.0\n",
      "1271    0.2\n",
      "Name: sentiment_score, Length: 1272, dtype: float64\n",
      "0       1.600000e+00\n",
      "1       1.600000e+00\n",
      "2       2.000000e-01\n",
      "3       4.200000e+00\n",
      "4       2.200000e+00\n",
      "            ...     \n",
      "1269    4.400000e+00\n",
      "1270    1.110223e-16\n",
      "1271    1.800000e+00\n",
      "1272    0.000000e+00\n",
      "1273    5.640000e+00\n",
      "Name: sentiment_score, Length: 1274, dtype: float64\n",
      "0       0.6\n",
      "1       0.6\n",
      "2       1.2\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "3811    0.6\n",
      "3812   -0.4\n",
      "3813    4.8\n",
      "3814   -0.2\n",
      "3815    0.6\n",
      "Name: sentiment_score, Length: 3816, dtype: float64\n",
      "0      -1.40\n",
      "1       0.00\n",
      "2      -0.40\n",
      "3       0.00\n",
      "4      -0.80\n",
      "        ... \n",
      "1267    6.20\n",
      "1268    4.10\n",
      "1269    1.16\n",
      "1270    2.60\n",
      "1271    0.00\n",
      "Name: sentiment_score, Length: 1272, dtype: float64\n",
      "0       0.00\n",
      "1       0.00\n",
      "2       1.00\n",
      "3      -0.20\n",
      "4       3.26\n",
      "        ... \n",
      "1269    0.00\n",
      "1270   -2.60\n",
      "1271    0.60\n",
      "1272    1.40\n",
      "1273    0.00\n",
      "Name: sentiment_score, Length: 1274, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "datasets_ch = ['Weibo-16', 'Weibo-16-original',\n",
    "               'Weibo-20', 'Weibo-20-temporal']\n",
    "dataset_dir = '/home/alissa77/WWW2021/code/preprocess/data'\n",
    "dataset_dir_new = '/home/alissa77/fake-news-detect/preprocess_data'\n",
    "tmp = []\n",
    "dic = {}\n",
    "dic2 = {}\n",
    "arrs = ['train', 'val', 'test']\n",
    "\n",
    "## data_a\n",
    "for dataset in datasets_ch : \n",
    "    train_data, val_data, test_data = [], [], []\n",
    "    data_dir_new = os.path.join(dataset_dir_new, dataset)  \n",
    "    for t in ['train', 'val', 'test']:\n",
    "        name1 = data_dir_new.split('/')[5]+\"_\"+t\n",
    "        dic2[name1] = pd.read_pickle(os.path.join(data_dir_new, '{}.pkl'.format(t)))['sentiment_score'] \n",
    "        print(dic2[name1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       0.6\n",
      "1       0.6\n",
      "2       1.2\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "2206    2.0\n",
      "2207   -0.6\n",
      "2208    1.0\n",
      "2209    0.0\n",
      "2210    0.0\n",
      "Name: sentiment_score, Length: 2211, dtype: float64\n",
      "0      3.40\n",
      "1      0.00\n",
      "2      1.60\n",
      "3      5.60\n",
      "4     -0.80\n",
      "       ... \n",
      "733   -0.40\n",
      "734   -0.28\n",
      "735    2.00\n",
      "736    1.44\n",
      "737    0.00\n",
      "Name: sentiment_score, Length: 738, dtype: float64\n",
      "0      0.0\n",
      "1      0.8\n",
      "2      0.0\n",
      "3      4.8\n",
      "4      3.8\n",
      "      ... \n",
      "752    2.8\n",
      "753    0.0\n",
      "754    0.2\n",
      "755   -0.8\n",
      "756    0.8\n",
      "Name: sentiment_score, Length: 757, dtype: float64\n",
      "0       0.60\n",
      "1       0.60\n",
      "2       4.96\n",
      "3       1.20\n",
      "4       0.00\n",
      "        ... \n",
      "2791    2.00\n",
      "2792   -0.60\n",
      "2793    1.00\n",
      "2794    0.00\n",
      "2795    0.00\n",
      "Name: sentiment_score, Length: 2796, dtype: float64\n",
      "0      0.00\n",
      "1      0.00\n",
      "2      0.00\n",
      "3      0.00\n",
      "4      3.40\n",
      "       ... \n",
      "928   -0.40\n",
      "929   -0.28\n",
      "930    2.00\n",
      "931    1.44\n",
      "932    0.00\n",
      "Name: sentiment_score, Length: 933, dtype: float64\n",
      "0      0.0\n",
      "1     -8.0\n",
      "2      0.0\n",
      "3      5.6\n",
      "4      0.0\n",
      "      ... \n",
      "929    2.8\n",
      "930    0.0\n",
      "931    0.2\n",
      "932   -0.8\n",
      "933    0.8\n",
      "Name: sentiment_score, Length: 934, dtype: float64\n",
      "0       1.00\n",
      "1       0.00\n",
      "2      -1.40\n",
      "3       5.20\n",
      "4       1.20\n",
      "        ... \n",
      "3811    1.40\n",
      "3812    0.66\n",
      "3813    0.00\n",
      "3814    0.20\n",
      "3815    0.00\n",
      "Name: sentiment_score, Length: 3816, dtype: float64\n",
      "0       0.0\n",
      "1       0.0\n",
      "2       1.0\n",
      "3      -0.8\n",
      "4       0.0\n",
      "       ... \n",
      "1267    0.0\n",
      "1268    0.2\n",
      "1269    0.0\n",
      "1270    0.0\n",
      "1271    0.2\n",
      "Name: sentiment_score, Length: 1272, dtype: float64\n",
      "0       1.600000e+00\n",
      "1       1.600000e+00\n",
      "2       2.000000e-01\n",
      "3       4.200000e+00\n",
      "4       2.200000e+00\n",
      "            ...     \n",
      "1269    4.400000e+00\n",
      "1270    1.110223e-16\n",
      "1271    1.800000e+00\n",
      "1272    0.000000e+00\n",
      "1273    5.640000e+00\n",
      "Name: sentiment_score, Length: 1274, dtype: float64\n",
      "0       0.6\n",
      "1       0.6\n",
      "2       1.2\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "3811    0.6\n",
      "3812   -0.4\n",
      "3813    4.8\n",
      "3814   -0.2\n",
      "3815    0.6\n",
      "Name: sentiment_score, Length: 3816, dtype: float64\n",
      "0      -1.40\n",
      "1       0.00\n",
      "2      -0.40\n",
      "3       0.00\n",
      "4      -0.80\n",
      "        ... \n",
      "1267    6.20\n",
      "1268    4.10\n",
      "1269    1.16\n",
      "1270    2.60\n",
      "1271    0.00\n",
      "Name: sentiment_score, Length: 1272, dtype: float64\n",
      "0       0.00\n",
      "1       0.00\n",
      "2       1.00\n",
      "3      -0.20\n",
      "4       3.26\n",
      "        ... \n",
      "1269    0.00\n",
      "1270   -2.60\n",
      "1271    0.60\n",
      "1272    1.40\n",
      "1273    0.00\n",
      "Name: sentiment_score, Length: 1274, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dataset_dir_new = '/home/alissa77/fake-news-detect/preprocess_data'\n",
    "\n",
    "for dataset in datasets_ch : \n",
    "    data_dir_new = os.path.join(dataset_dir_new, dataset)  \n",
    "    split_datasets = [pd.read_pickle(os.path.join(data_dir_new, '{}.pkl'.format(t))) for t in ['train', 'val', 'test']] \n",
    "    split_datasets = dict(zip(['train', 'val', 'test'], split_datasets))\n",
    "    for t, pieces in split_datasets.items(): \n",
    "        print(pieces['sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_NUM = 150 # maximum number of sentences in one document\n",
    "MAX_WORD_NUM = 150     # maximum number of words in each sentence\n",
    "EMBED_SIZE = 768      # vector size of word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU, TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "\n",
    "class EmotionEnhancedBiGRU:\n",
    "    def __init__(self, max_sequence_length, embedding_matrix, emotion_dim=0, category_num=2, hidden_units=32, l2_param=0.01, lr_param=0.001):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.emotion_dim = emotion_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.category_num = category_num\n",
    "        self.l2_param = l2_param\n",
    "\n",
    "        self.model = self.build()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=Adam(\n",
    "            lr=lr_param, beta_1=0.8), metrics=['accuracy'])\n",
    "\n",
    "    def build(self):\n",
    "        semantic_input = Input(\n",
    "            shape=(self.max_sequence_length,), name='word-embedding-input')\n",
    "\n",
    "        semantic_emb = Embedding(self.embedding_matrix.shape[0],\n",
    "                                 self.embedding_matrix.shape[1],\n",
    "                                 embeddings_initializer=Constant(\n",
    "                                     self.embedding_matrix),\n",
    "                                 input_length=self.max_sequence_length,\n",
    "                                 trainable=False)(semantic_input)\n",
    "\n",
    "        gru = Bidirectional(\n",
    "            GRU(self.hidden_units, return_sequences=True))(semantic_emb)\n",
    "        avg_pool = GlobalAveragePooling1D()(gru)\n",
    "\n",
    "        if self.emotion_dim != 0:\n",
    "            emotion_input = Input(\n",
    "                shape=(self.emotion_dim,), name='emotion-input')\n",
    "            emotion_enhanced = Concatenate()([avg_pool, emotion_input])\n",
    "\n",
    "            dense = Dense(32, activation='relu', kernel_regularizer=l2(\n",
    "                self.l2_param))(emotion_enhanced)\n",
    "            output = Dense(self.category_num, activation='softmax',\n",
    "                           kernel_regularizer=l2(self.l2_param))(dense)\n",
    "            model = Model(\n",
    "                inputs=[semantic_input, emotion_input], outputs=output)\n",
    "        else:\n",
    "            dense = Dense(32, activation='relu',\n",
    "                          kernel_regularizer=l2(self.l2_param))(avg_pool)\n",
    "            output = Dense(self.category_num, activation='softmax',\n",
    "                           kernel_regularizer=l2(self.l2_param))(dense)\n",
    "            model = Model(inputs=[semantic_input], outputs=output)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(review,stopWords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = tokenize.sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence) \n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    \n",
    "    return returnString, idx_list\n",
    "\n",
    "\n",
    "def split_df(dataframe, column_name, training_split = 0.6, validation_split = 0.2, test_split = 0.2):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into trainingset, validationset and testset in specified ratio.\n",
    "    All sets are balanced, which means they have the same ratio for each categorie as the full set.\n",
    "    Input:   dataframe        - Pandas Dataframe, should include a column for data and one for categories\n",
    "             column_name      - Name of dataframe column which contains the categorical output values\n",
    "             training_split   - from ]0,1[, default = 0.6\n",
    "             validation_split - from ]0,1[, default = 0.2        \n",
    "             test_split       - from ]0,1[, default = 0.2\n",
    "                                Sum of all splits need to be 1\n",
    "    Output:  train            - Pandas DataFrame of trainset\n",
    "             validation       - Pandas DataFrame of validationset\n",
    "             test             - Pandas DataFrame of testset\n",
    "    \"\"\"\n",
    "    if training_split + validation_split + test_split != 1.0:\n",
    "        raise ValueError('Split paramter sum should be 1.0')\n",
    "        \n",
    "    total = len(dataframe.index)\n",
    " \n",
    "    train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
    "    .reset_index(drop=True).set_index('index')\n",
    "    train = train.sample(frac=1)\n",
    "    temp_df = dataframe.drop(train.index)\n",
    "    validation = temp_df.reset_index().groupby(column_name)\\\n",
    "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
    "           .reset_index(drop=True).set_index('index')\n",
    "    validation = validation.sample(frac=1)\n",
    "    test = temp_df.drop(validation.index)\n",
    "    test = test.sample(frac=1)\n",
    "    \n",
    "    print('Total: ', len(dataframe))\n",
    "    print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
    "    print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
    "    print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "def wordToSeq(text,word_index,max_sentences,max_words,max_features):\n",
    "    \"\"\"\n",
    "    Converts a string to a numpy matrix where each word is tokenized.\n",
    "    Arrays are zero-padded to max_sentences and max_words length.\n",
    "    \n",
    "    Input:    text           - string of sentences\n",
    "              word_index     - trained word_index\n",
    "              max_sentences  - maximum number of sentences allowed per document for HAN\n",
    "              max_words      - maximum number of words in each sentence for HAN\n",
    "              max_features   - maximum number of unique words to be tokenized\n",
    "    Output:   data           - Numpy Matrix of size [max_sentences x max_words]\n",
    "    \"\"\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    data = np.zeros((max_sentences, max_words), dtype='int32')\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j< max_sentences:\n",
    "            wordTokens = tokenize.word_tokenize(sent.rstrip('.'))\n",
    "            wordTokens = [w for w in wordTokens]\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k<max_words and word_index[word]<max_features:\n",
    "                        data[j,k] = word_index[word]\n",
    "                        k=k+1\n",
    "                except:\n",
    "                    pass\n",
    "    return data\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self,attention_dim=768,return_coefficients=False,**kwargs):\n",
    "    #def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Builds all weights\n",
    "        # W = Weight matrix, b = bias vector, u = context vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_lyrics = df.WS.to_dict()\n",
    "#train_lyrics = np.load(\"train_WS.npy\",allow_pickle=True)\n",
    "\n",
    "# 程度副詞 #md\n",
    "degree = {'最':0.9, '最為':0.9, '極':0.9, '極為':0.9, '極其':0.9, '極度':0.9, '極端':0.9, '至為':0.9,\\\n",
    "           '頂':0.9, '過':0.9, '過於':0.9, '過分':0.9, '分外':0.9, '萬分':0.9,\"全\":0.9,\\\n",
    "           '更':0.7, '更加':0.7, '更為':0.7, '更其':0.7, '越':0.7, '越發':0.7, '備加':0.7, '愈加':0.7, '愈':0.7,\\\n",
    "           '愈發':0.7, '愈為':0.7, '愈益':0.7, '越加':0.7, '格':0.7, '益發':0.7, '還':0.7, '很':0.7, '太':0.7,'都':0.7,\\\n",
    "           '挺':0.7, '怪':0.7, '老是':0.7, '非常':0.7, '特別':0.7, '相當':0.7, '十分':0.7, '好':0.7, '好不':0.9,\\\n",
    "           '甚':0.7, '甚為':0.7, '頗':0.7, '頗為':0.7, '滿':0.7, '蠻':0.7, '夠':0.7, '多':0.7,\\\n",
    "           '多麼':0.7, '特':0.7, '大':0.7, '大為':0.7, '何等':0.7, '何其':0.7, '尤其':0.7, '無比':0.7, '尤為':-0.7,\\\n",
    "           '較':0.5, '比較':0.5, '較比':0.5, '較為':0.5, '還':0.5, '不大':-0.5, '不太':0.5, '不很':0.5, '不甚':0.5,\\\n",
    "           '早已':0.7,'嗎':0.5,'大多':-0.5,'超級':0.7,\\\n",
    "           '稍':-0.5, '稍稍':-0.5, '稍為':-0.5, '稍微':-0.5, '稍許':-0.5, '親自':-0.5, '略':-0.5, '略為':-0.5,\\\n",
    "           '些微':-0.5, '多少':-0.5, '有點':-0.7, '有點兒':-0.5, '有些':-0.5, '多為':-0.7,'！':0.7}\n",
    "\n",
    "# 否定副詞 #mn\n",
    "negative = {'白':-0.8, '白白':-0.8, '甭':-0.8, '別':-0.8, '不':-0.8, '不必':-0.8, '不曾':-0.8, '不太':-0.8,'不要':-0.8,\\\n",
    "          '不用':-0.8, '非':-0.8, '幹':-0.8, '何必':-0.8, '何曾':-0.8, '何嘗':-0.8, '何須':-0.8,\\\n",
    "          '空':-0.8, '沒':-0.8, '沒有':-0.8, '莫':-0.8, '徒':-0.8, '徒然':-0.8, '忹':-0.8,\\\n",
    "          '未':-0.8, '未曾':-0.8, '未嘗':-0.8, '無須':-0.8, '無須乎':-0.8, '無需':-0.8, '毋須':-0.8,\\\n",
    "          '毋庸':-0.8, '無庸':-0.8, '勿':-0.8, '瞎':-0.8, '休':-0.8, '虛':-0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def sesntiment_score(lyrics,dataframe):\n",
    "    def b_edge(value):\n",
    "        if value<0:\n",
    "            return -4\n",
    "        else:\n",
    "            return 4\n",
    "    lst=[]\n",
    "    valence = {}\n",
    "    arousal = {}\n",
    "    with open('cvaw4.csv', newline='') as csvfile:\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows:\n",
    "            valence[row['Word']] = (row['Valence_Mean'])\n",
    "            arousal[row['Word']] = (row['Arousal_Mean'])\n",
    "\n",
    "    valence = {x: round(float(valence[x])-5, 2) for x in valence}\n",
    "    arousal = {x: round(float(arousal[x])-5, 2) for x in arousal}\n",
    "\n",
    "    for ID in lyrics:\n",
    "        score_v = 0\n",
    "        score_a = 0\n",
    "        score_degree = 0\n",
    "        score_negative = 0\n",
    "        len_cvaw = 1 \n",
    "\n",
    "        for i in range(2, len(lyrics[ID])): # lyric[i] = a word in CVAW dictionary\n",
    "            lyric = lyrics[ID]\n",
    "            temp_v = 0  # vep\n",
    "            temp_a = 0  # aep\n",
    "\n",
    "            if  lyrics[ID] in list(degree):\n",
    "                score_degree += degree[ lyrics[ID]]\n",
    "            if  lyrics[ID] in list(negative):\n",
    "                score_negative += negative[ lyrics[ID]]\n",
    "\n",
    "            if lyric[i] in arousal:\n",
    "                len_cvaw += 1\n",
    "                this_v = valence[lyric[i]]  # Vew\n",
    "                this_a = arousal[lyric[i]]  # Aew\n",
    "\n",
    "\n",
    "                if lyric[i-2] in negative:\n",
    "                    if lyric[i-1] in negative:\n",
    "                        print (\"N + N + EW:\", lyric[i-2:i+1:])\n",
    "                        param = negative[ lyric[i-2] ] * negative[ lyric[i-1] ]\n",
    "                        temp_v = param * this_v\n",
    "                        temp_a = param * this_a\n",
    "\n",
    "                    elif lyric[i-1] in degree:\n",
    "                        print (\"N + D + EW:\", lyric[i-2:i+1:])\n",
    "                        param = degree[ lyric[i-1] ] - (1 + negative[ lyric[i-2] ])\n",
    "                        temp_v = this_v + (b_edge(this_v) - this_v) * param\n",
    "                        temp_a = this_a + (b_edge(this_a) - this_a) * param\n",
    "\n",
    "                elif lyric[i-2] in degree:\n",
    "                    if lyric[i-1] in negative:\n",
    "                        print (\"D + N + EW:\", lyric[i-2:i+1:])\n",
    "                        mn = negative[ lyric[i-1] ]\n",
    "                        md = degree[ lyric[i-2] ]\n",
    "                        param_v = mn * this_v\n",
    "                        param_a = mn * this_a\n",
    "                        temp_v = param_v + (b_edge(this_v) - param_v) * md\n",
    "                        temp_a = param_a + (b_edge(this_a) - param_a) * md\n",
    "\n",
    "                    elif lyric[i-1] in degree:\n",
    "                        print (\"D + D + EW:\", lyric[i-2:i+1:])\n",
    "                        md_1 = degree[ lyric[i-1] ]\n",
    "                        md_2 = degree[ lyric[i-2] ]\n",
    "                        param_v = (b_edge(this_v) - this_v) * md_1\n",
    "                        param_a = (b_edge(this_a) - this_a) * md_1\n",
    "                        temp_v = this_v + param_v + (1 - (this_v + param_v)) * md_2\n",
    "                        temp_a = this_a + param_a + (1 - (this_a + param_a)) * md_2\n",
    "\n",
    "                elif lyric[i-1] in negative:\n",
    "                    print (\"N + EW:\", lyric[i-1:i+1:])\n",
    "                    temp_v = negative[ lyric[i-1] ] * valence[lyric[i]]\n",
    "                    temp_a = negative[ lyric[i-1] ] * arousal[lyric[i]]\n",
    "\n",
    "                elif lyric[i-1] in degree:\n",
    "                    print (\"D + EW:\", lyric[i-1:i+1:])\n",
    "                    temp_v = this_v + (b_edge(this_v) - this_v) * degree[ lyric[i-1] ]\n",
    "                    temp_a = this_a + (b_edge(this_a) - this_a) * degree[ lyric[i-1] ]\n",
    "                \n",
    "                else:\n",
    "                    print (\"EW:\", lyric[i])   \n",
    "                    temp_v = valence[lyric[i]]\n",
    "                    temp_a = arousal[lyric[i]]\n",
    "                \n",
    "\n",
    "            score_v += temp_v\n",
    "            score_a += temp_a     \n",
    "\n",
    "        valence_output = score_v/len_cvaw\n",
    "        \n",
    "        # 排除情況（第四象限\n",
    "        if (score_a < 0 and valence_output == 0):\n",
    "            score_a = 0         \n",
    "        \n",
    "        lst.append(score_a)\n",
    "        new_emotion_word = pd.Series(lst)   # add new emotion list\n",
    "        dataframe['sentiment_score'] = new_emotion_word\n",
    "        \n",
    "        #print (\"Mean_valence:\", valence_output)\n",
    "        print(lyrics[ID])\n",
    "        print (\"Mean_arousal:\", score_a)\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033, 1)\n",
      "(4222, 1)\n"
     ]
    }
   ],
   "source": [
    "#padding\n",
    "\n",
    "train_ss = np.asarray(list(data[\"sentiment_score\"]))\n",
    "test_ss = np.asarray(list(df[\"sentiment_score\"]))\n",
    "train_ss = np.expand_dims(train_ss, axis=1)\n",
    "test_ss = np.expand_dims(test_ss, axis=1)\n",
    "#train_ss = np.expand_dims(y, axis=2)\n",
    "#test_ss = np.expand_dims(y1, axis=2)\n",
    "#train_ss = pad_sequences(x, maxlen = 150, padding = 'post',dtype = \"float32\")\n",
    "#test_ss = pad_sequences(x1, maxlen = 150, padding = 'post',dtype = \"float32\")\n",
    "print(test_ss.shape)\n",
    "print(train_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6],\n",
       "       [0. ],\n",
       "       [0.8],\n",
       "       ...,\n",
       "       [1.6],\n",
       "       [0.8],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "datasets_ch = ['Weibo-16', 'Weibo-16-original',\n",
    "               'Weibo-20', 'Weibo-20-temporal']\n",
    "\n",
    "# ============================== Category ==============================\n",
    "baidu_emotions = ['angry', 'disgusting', 'fearful',\n",
    "                  'happy', 'sad', 'neutral', 'pessimistic', 'optimistic']\n",
    "baidu_emotions.sort()\n",
    "\n",
    "baidu_emotions_2_index = dict(zip(baidu_emotions, [i for i in range(len(baidu_emotions))]))\n",
    "\n",
    "\n",
    "def baidu_arr(emotions_dict):\n",
    "    arr = np.zeros(len(baidu_emotions))\n",
    "\n",
    "    if emotions_dict is None:\n",
    "        return arr\n",
    "\n",
    "    for k, v in emotions_dict.items():\n",
    "        # like -> happy\n",
    "        if k == 'like':\n",
    "            arr[baidu_emotions_2_index['happy']] += v\n",
    "        else:\n",
    "            arr[baidu_emotions_2_index[k]] += v\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "# load negation words\n",
    "negation_words = []\n",
    "with open('/home/alissa77/WWW2021/resources/Chinese/others/negative/negationWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        negation_words.append(line.strip())\n",
    "\n",
    "print('\\nThe num of negation words: ', len(negation_words))\n",
    "\n",
    "# load degree words\n",
    "how_words_dict = dict()\n",
    "with open('/home/alissa77/WWW2021/resources/Chinese/HowNet/intensifierWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        how_word = line.strip().split()\n",
    "        how_words_dict[' '.join(how_word[:-1])] = float(how_word[-1])\n",
    "\n",
    "print('The num of degree words: ', len(how_words_dict),\n",
    "      '. eg: ', list(how_words_dict.items())[0])\n",
    "\n",
    "\n",
    "# negation value and degree value\n",
    "def get_not_and_how_value(cut_words, i, windows):\n",
    "    not_cnt = 0\n",
    "    how_v = 1\n",
    "\n",
    "    left = 0 if (i - windows) < 0 else (i - windows)\n",
    "    window_text = ' '.join(cut_words[left:i])\n",
    "\n",
    "    for w in negation_words:\n",
    "        if w in window_text:\n",
    "            not_cnt += 1\n",
    "    for w in how_words_dict.keys():\n",
    "        if w in window_text:\n",
    "            how_v *= how_words_dict[w]\n",
    "\n",
    "    # for w in cut_words[left:i]:\n",
    "    #     if w in negation_words:\n",
    "    #         not_cnt += 1\n",
    "    #     if w in how_words_dict:\n",
    "    #         how_v *= how_words_dict[w]\n",
    "\n",
    "    return (-1) ** not_cnt, how_v\n",
    "\n",
    "\n",
    "_, words2array = joblib.load(\n",
    "    '/home/alissa77/WWW2021/resources/Chinese/大连理工大学情感词汇本体库/preprocess/words2array_27351.pkl')\n",
    "print('[Dalianligong]\\tThere are {} words, the dimension is {}'.format(\n",
    "    len(words2array), words2array['快乐'].shape))\n",
    "\n",
    "\n",
    "def dalianligong_arr(cut_words, windows=2):\n",
    "    arr = np.zeros(29)\n",
    "\n",
    "    for i, word in enumerate(cut_words):\n",
    "        if word in words2array:\n",
    "            not_v, how_v = get_not_and_how_value(cut_words, i, windows)\n",
    "            arr += not_v * how_v * words2array[word]\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================== Auxilary Features ==============================\n",
    "\n",
    "\n",
    "# Emoticon\n",
    "emoticon_df = pd.read_csv(\n",
    "    '/home/alissa77/WWW2021/resources/Chinese/others/emoticon/emoticon.csv')\n",
    "emoticons = emoticon_df['emoticon'].tolist()\n",
    "emoticon_types = list(set(emoticon_df['label'].tolist()))\n",
    "emoticon_types.sort()\n",
    "emoticon2label = dict(\n",
    "    zip(emoticon_df['emoticon'].tolist(), emoticon_df['label'].tolist()))\n",
    "emoticon2index = dict(\n",
    "    zip(emoticon_types, [i for i in range(len(emoticon_types))]))\n",
    "\n",
    "print('[Emoticon]\\tThere are {} emoticons, including {} categories'.format(\n",
    "    len(emoticons), len(emoticon_types)))\n",
    "\n",
    "\n",
    "def emoticon_arr(text, cut_words):\n",
    "    arr = np.zeros(len(emoticon_types))\n",
    "\n",
    "    if len(cut_words) == 0:\n",
    "        return arr\n",
    "\n",
    "    for i, emoticon in enumerate(emoticons):\n",
    "        if emoticon in text:\n",
    "            arr[emoticon2index[emoticon2label[emoticon]]\n",
    "                ] += text.count(emoticon)\n",
    "\n",
    "    return arr / len(cut_words)\n",
    "\n",
    "\n",
    "# Punctuation\n",
    "def symbols_count(text):\n",
    "    excl = (text.count('!') + text.count('！')) / len(text)\n",
    "    ques = (text.count('?') + text.count('？')) / len(text)\n",
    "    comma = (text.count(',') + text.count('，')) / len(text)\n",
    "    dot = (text.count('.') + text.count('。')) / len(text)\n",
    "    ellip = (text.count('..') + text.count('。。')) / len(text)\n",
    "\n",
    "    return excl, ques, comma, dot, ellip\n",
    "\n",
    "\n",
    "# Sentimental Words\n",
    "def init_words(file):\n",
    "    with open(file, 'r', encoding='utf-8') as src:\n",
    "        words = src.readlines()\n",
    "        words = [l.strip() for l in words]\n",
    "    # print('File: {}, Words_sz = {}'.format(file.split('/')[-1], len(words)))\n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "pos_words = init_words('/home/alissa77/WWW2021/resources/Chinese/HowNet/正面评价词语（中文）.txt')\n",
    "pos_words += init_words('/home/alissa77/WWW2021/resources/Chinese/HowNet/正面评价词语（中文）.txt')\n",
    "neg_words = init_words('/home/alissa77/WWW2021/resources/Chinese/HowNet/负面情感词语（中文）.txt')\n",
    "neg_words += init_words('/home/alissa77/WWW2021/resources/Chinese/HowNet/负面评价词语（中文）.txt')\n",
    "\n",
    "pos_words = set(pos_words)\n",
    "neg_words = set(neg_words)\n",
    "print('[HowNet]\\tThere are {} positive words and {} negative words'.format(\n",
    "    len(pos_words), len(neg_words)))\n",
    "\n",
    "\n",
    "def sentiment_words_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "    # positive and negative words\n",
    "    sentiment = []\n",
    "    for words in [pos_words, neg_words]:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            if word in cut_words:\n",
    "                # print(word)\n",
    "                c += 1\n",
    "        sentiment.append(c)\n",
    "    sentiment = [c / len(cut_words) for c in sentiment]\n",
    "\n",
    "    # degree words\n",
    "    degree = 0\n",
    "    for word in how_words_dict:\n",
    "        if word in cut_words:\n",
    "            # print(word)\n",
    "            degree += how_words_dict[word]\n",
    "\n",
    "    # negation words\n",
    "    negation = 0\n",
    "    for word in negation_words:\n",
    "        negation += cut_words.count(word)\n",
    "    negation /= len(cut_words)\n",
    "\n",
    "    sentiment += [degree, negation]\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Personal Pronoun\n",
    "first_pronoun = init_words(\n",
    "    '/home/alissa77/WWW2021/resources/Chinese/others/pronoun/1-personal-pronoun.txt')\n",
    "second_pronoun = init_words(\n",
    "    '/home/alissa77/WWW2021/resources/Chinese/others/pronoun/2-personal-pronoun.txt')\n",
    "third_pronoun = init_words(\n",
    "    '/home/alissa77/WWW2021/resources/Chinese/others/pronoun/3-personal-pronoun.txt')\n",
    "pronoun_words = [first_pronoun, second_pronoun, third_pronoun]\n",
    "\n",
    "\n",
    "def pronoun_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "    pronoun = []\n",
    "    for words in pronoun_words:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            c += cut_words.count(word)\n",
    "        pronoun.append(c)\n",
    "\n",
    "    return [c / len(cut_words) for c in pronoun]\n",
    "\n",
    "\n",
    "# Auxilary Features\n",
    "def auxilary_features(text, cut_words):\n",
    "    arr = np.zeros(17)\n",
    "\n",
    "    arr[:5] = emoticon_arr(text, cut_words)\n",
    "    arr[5:10] = symbols_count(text)\n",
    "    arr[10:14] = sentiment_words_count(cut_words)\n",
    "    arr[14:17] = pronoun_count(cut_words)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def extract_publisher_emotion(claim, content_words):\n",
    "    text, cut_words = claim, content_words\n",
    "\n",
    "    arr = np.zeros(55)\n",
    "    arr[:8] = baidu_arr(emotion_dict)\n",
    "    arr[8:37] = dalianligong_arr(cut_words)\n",
    "    arr[37:54] = auxilary_features(text, cut_words)\n",
    "    arr[54:55] = piece['sentiment_score']\n",
    "    return arr\n",
    "\n",
    "def extract_dual_emotion(piece):\n",
    "    single_emotion = extract_publisher_emotion(piece['claim'], piece['content_words'])    \n",
    "    return single_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_POS = np.load(\"test_POS.npy\",allow_pickle=True)\n",
    "train_POS = np.load(\"train_POS.npy\",allow_pickle=True)\n",
    "testpos = pd.DataFrame(test_POS, columns = ['head_pos'])\n",
    "trainpos = pd.DataFrame(train_POS, columns = ['head_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pos_index = {}\n",
    "idx = 1\n",
    "\n",
    "for index, rows in trainpos.iterrows() :\n",
    "    for pos in rows['head_pos'] :\n",
    "        if pos not in pos_index :\n",
    "            pos_index[pos] = idx\n",
    "            idx += 1\n",
    "            \n",
    "for index, rows in testpos.iterrows() :\n",
    "    for pos in rows['head_pos'] :\n",
    "        if pos not in pos_index :\n",
    "            pos_index[pos] = idx\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_syn2index(head_syn) :\n",
    "    pos_list = []\n",
    "    for pos in head_syn :\n",
    "        pos_list.append(pos_index[pos])\n",
    "    \n",
    "    return pos_list\n",
    "\n",
    "\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    if 'axis' in kwargs:\n",
    "        axis = kwargs.pop('axis')\n",
    "        return df.apply(func, **kwargs, axis=axis)\n",
    "    \n",
    "    return df.apply(func, **kwargs)\n",
    "\n",
    "\n",
    "def apply_by_multiprocessing(df, func, **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    if workers == -1:\n",
    "        workers = multiprocessing.cpu_count() \n",
    "    coln = 1    \n",
    "    if 'coln' in kwargs:\n",
    "        coln = kwargs.pop('coln')\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs) for d in np.array_split(df, workers)])\n",
    "    pool.close()\n",
    "    series = pd.concat(list(result))\n",
    "\n",
    "    if coln == 1:\n",
    "        return series\n",
    "    elif coln == 2:\n",
    "        series_0, series_1 = series.apply(lambda x: x[0]), series.apply(lambda x: x[1])\n",
    "        return series_0, series_1\n",
    "    elif coln == 3:\n",
    "        series_0, series_1, series_2 = series.apply(lambda x: x[0]), series.apply(lambda x: x[1]), series.apply(lambda x: x[2])\n",
    "        \n",
    "        return series_0, series_1, series_2  \n",
    "    \n",
    "    \n",
    "def trainset_create(head_pos_index, head_max_len, body_max_article_len) : \n",
    "    all_head_pos = []\n",
    "    head_pos_index = pad_sequences(head_pos_index, maxlen = head_max_len, padding = 'post')\n",
    "    for pos_list in head_pos_index :\n",
    "        pos_list = np.array(pos_list)\n",
    "        pos_list = pos_list.reshape(1,150)\n",
    "\n",
    "        all_head_pos.append(np.repeat(pos_list, repeats=body_max_article_len, axis=0))\n",
    "    all_head_pos = np.array(all_head_pos)\n",
    "     \n",
    "    return all_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_pos</th>\n",
       "      <th>head_pos_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Nb, VA, VA, VC, VH, VH, Nc, P, Na, VH, Nc, Nb...</td>\n",
       "      <td>[5, 3, 3, 7, 12, 12, 1, 16, 2, 12, 1, 5, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nc, Nc, D, VC, DE, Na, Na, VC, Na, Neu, Nf, V...</td>\n",
       "      <td>[1, 1, 6, 7, 31, 2, 2, 7, 2, 4, 14, 12, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Nd, Nd, Nb, WHITESPACE, VH, VH, Neu, Nf, VC, ...</td>\n",
       "      <td>[9, 9, 5, 52, 12, 12, 4, 14, 7, 4, 14, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Nd, Na, D, VH, EXCLAMATIONCATEGORY, WHITESPAC...</td>\n",
       "      <td>[9, 2, 6, 12, 27, 52, 12, 12, 4, 1, 52, 7, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Nd, Nes, Nf, Nb, Na, Na, WHITESPACE, Nb, VC, ...</td>\n",
       "      <td>[9, 15, 14, 5, 2, 2, 52, 5, 7, 21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>[Na, Na, Nh, VK, Na, Na, Nep, Nf, Na, Nep, D, ...</td>\n",
       "      <td>[2, 2, 35, 11, 2, 2, 45, 14, 2, 45, 6, 6, 6, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>[Neu, Na, VA, VA, VF, VC, Na, EXCLAMATIONCATEG...</td>\n",
       "      <td>[4, 2, 3, 3, 24, 7, 2, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>[Na, A, VC, D, VC, VH, VD, Di, Na]</td>\n",
       "      <td>[2, 30, 7, 6, 7, 12, 13, 37, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>[Nb, P, Nc, Na, VC, DE, Na, Ncd, V_2, Neu, Nf,...</td>\n",
       "      <td>[5, 16, 1, 2, 7, 31, 2, 36, 28, 4, 14, 2, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>[FW, VE, Nd, SHI, Na, VH, DE, Na, Na, SHI, A, ...</td>\n",
       "      <td>[10, 18, 9, 39, 2, 12, 31, 2, 2, 39, 30, 4, 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1033 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               head_pos  \\\n",
       "0     [Nb, VA, VA, VC, VH, VH, Nc, P, Na, VH, Nc, Nb...   \n",
       "1     [Nc, Nc, D, VC, DE, Na, Na, VC, Na, Neu, Nf, V...   \n",
       "2     [Nd, Nd, Nb, WHITESPACE, VH, VH, Neu, Nf, VC, ...   \n",
       "3     [Nd, Na, D, VH, EXCLAMATIONCATEGORY, WHITESPAC...   \n",
       "4     [Nd, Nes, Nf, Nb, Na, Na, WHITESPACE, Nb, VC, ...   \n",
       "...                                                 ...   \n",
       "1028  [Na, Na, Nh, VK, Na, Na, Nep, Nf, Na, Nep, D, ...   \n",
       "1029  [Neu, Na, VA, VA, VF, VC, Na, EXCLAMATIONCATEG...   \n",
       "1030                 [Na, A, VC, D, VC, VH, VD, Di, Na]   \n",
       "1031  [Nb, P, Nc, Na, VC, DE, Na, Ncd, V_2, Neu, Nf,...   \n",
       "1032  [FW, VE, Nd, SHI, Na, VH, DE, Na, Na, SHI, A, ...   \n",
       "\n",
       "                                         head_pos_index  \n",
       "0     [5, 3, 3, 7, 12, 12, 1, 16, 2, 12, 1, 5, 16, 1...  \n",
       "1            [1, 1, 6, 7, 31, 2, 2, 7, 2, 4, 14, 12, 2]  \n",
       "2             [9, 9, 5, 52, 12, 12, 4, 14, 7, 4, 14, 2]  \n",
       "3     [9, 2, 6, 12, 27, 52, 12, 12, 4, 1, 52, 7, 4, ...  \n",
       "4                    [9, 15, 14, 5, 2, 2, 52, 5, 7, 21]  \n",
       "...                                                 ...  \n",
       "1028  [2, 2, 35, 11, 2, 2, 45, 14, 2, 45, 6, 6, 6, 3...  \n",
       "1029                         [4, 2, 3, 3, 24, 7, 2, 27]  \n",
       "1030                    [2, 30, 7, 6, 7, 12, 13, 37, 2]  \n",
       "1031  [5, 16, 1, 2, 7, 31, 2, 36, 28, 4, 14, 2, 2, 3...  \n",
       "1032  [10, 18, 9, 39, 2, 12, 31, 2, 2, 39, 30, 4, 14...  \n",
       "\n",
       "[1033 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpos['head_pos_index'] = apply_by_multiprocessing(testpos['head_pos'], head_syn2index, workers = -1)\n",
    "trainpos['head_pos_index'] = apply_by_multiprocessing(trainpos['head_pos'], head_syn2index, workers = -1)\n",
    "testpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033, 150, 150)\n",
      "(4222, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "test_head_pos = trainset_create(list(testpos['head_pos_index']),150, 150)\n",
    "train_head_pos = trainset_create(list(trainpos['head_pos_index']),150, 150)\n",
    "\n",
    "print(test_head_pos.shape)\n",
    "print(train_head_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5,  3,  3, ...,  0,  0,  0],\n",
       "        [ 5,  3,  3, ...,  0,  0,  0],\n",
       "        [ 5,  3,  3, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 5,  3,  3, ...,  0,  0,  0],\n",
       "        [ 5,  3,  3, ...,  0,  0,  0],\n",
       "        [ 5,  3,  3, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 1,  1,  6, ...,  0,  0,  0],\n",
       "        [ 1,  1,  6, ...,  0,  0,  0],\n",
       "        [ 1,  1,  6, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1,  1,  6, ...,  0,  0,  0],\n",
       "        [ 1,  1,  6, ...,  0,  0,  0],\n",
       "        [ 1,  1,  6, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 9,  9,  5, ...,  0,  0,  0],\n",
       "        [ 9,  9,  5, ...,  0,  0,  0],\n",
       "        [ 9,  9,  5, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 9,  9,  5, ...,  0,  0,  0],\n",
       "        [ 9,  9,  5, ...,  0,  0,  0],\n",
       "        [ 9,  9,  5, ...,  0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2, 30,  7, ...,  0,  0,  0],\n",
       "        [ 2, 30,  7, ...,  0,  0,  0],\n",
       "        [ 2, 30,  7, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 2, 30,  7, ...,  0,  0,  0],\n",
       "        [ 2, 30,  7, ...,  0,  0,  0],\n",
       "        [ 2, 30,  7, ...,  0,  0,  0]],\n",
       "\n",
       "       [[ 5, 16,  1, ...,  0,  0,  0],\n",
       "        [ 5, 16,  1, ...,  0,  0,  0],\n",
       "        [ 5, 16,  1, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 5, 16,  1, ...,  0,  0,  0],\n",
       "        [ 5, 16,  1, ...,  0,  0,  0],\n",
       "        [ 5, 16,  1, ...,  0,  0,  0]],\n",
       "\n",
       "       [[10, 18,  9, ...,  0,  0,  0],\n",
       "        [10, 18,  9, ...,  0,  0,  0],\n",
       "        [10, 18,  9, ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [10, 18,  9, ...,  0,  0,  0],\n",
       "        [10, 18,  9, ...,  0,  0,  0],\n",
       "        [10, 18,  9, ...,  0,  0,  0]]], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_head_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3377, 1)\n",
      "(3377, 150, 768)\n",
      "(3377,)\n",
      "(845, 150, 768)\n",
      "(845,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "\n",
    "y = data['label']\n",
    "X = train\n",
    "x_train_metadata = train_head_pos\n",
    "\n",
    "#train_ss is (4022,1)\n",
    "x_train, x_val, x_metadata, x_metadata_val, x_ss, x_ss_val, y_train, y_val = train_test_split(\n",
    "                            X, x_train_metadata, train_ss, y, random_state = 42, test_size = 0.2, shuffle = True)\n",
    "\n",
    "print(x_ss.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/alissa77/fake-news-detect/WWW2021/word-embedding/sgns.weibo.bigram-char, there are 195197 vectors\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/alissa77/fake-news-detect/WWW2021/word-embedding')\n",
    "\n",
    "def load_embeddings(language, embeddings_file):\n",
    "    assert language in ['Chinese', 'English']\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(embeddings_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "        lines = lines if language == 'English' else lines[1:]\n",
    "\n",
    "        for line in lines:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('File: {}, there are {} vectors'.format(\n",
    "        embeddings_file, len(embeddings_index)))\n",
    "    return embeddings_index\n",
    "\n",
    "# embedding\n",
    "MAX_NUM_WORDS = 6000\n",
    "embeddings_index = load_embeddings(language='Chinese', embeddings_file='/home/alissa77/fake-news-detect/WWW2021/word-embedding/sgns.weibo.bigram-char')\n",
    "CONTENT_WORDS = 100\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))\n",
    "\n",
    "content_arr = pad_sequences(sequences, maxlen=CONTENT_WORDS, padding='post')\n",
    "print('Content Array: {}'.format(content_arr.shape))\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.random.randn(num_words, EMBEDDING_DIM)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Embedding Matrix: {}'.format(embedding_matrix.shape))\n",
    "\n",
    "np.save('embedding_matrix.npy')\n",
    "\n",
    "for i, t in enumerate(['train', 'val', 'test']):\n",
    "    np.save(os.path.join(output_dir, '{}_{}.npy'.format(t, arrs[i].shape)), arrs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import GRU, TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "\n",
    "class EmotionEnhancedBiGRU:\n",
    "    def __init__(self, max_sequence_length, embedding_matrix, emotion_dim=0, category_num=2, hidden_units=32, l2_param=0.01, lr_param=0.001):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.emotion_dim = emotion_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.category_num = category_num\n",
    "        self.l2_param = l2_param\n",
    "\n",
    "        self.model = self.build()\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=Adam(\n",
    "            lr=lr_param, beta_1=0.8), metrics=['accuracy'])\n",
    "\n",
    "    def build(self):\n",
    "        semantic_input = Input(\n",
    "            shape=(self.max_sequence_length,), name='word-embedding-input')\n",
    "\n",
    "        semantic_emb = Embedding(self.embedding_matrix.shape[0],\n",
    "                                 self.embedding_matrix.shape[1],\n",
    "                                 embeddings_initializer=Constant(\n",
    "                                     self.embedding_matrix),\n",
    "                                 input_length=self.max_sequence_length,\n",
    "                                 trainable=False)(semantic_input)\n",
    "\n",
    "        gru = Bidirectional(\n",
    "            GRU(self.hidden_units, return_sequences=True))(semantic_emb)\n",
    "        avg_pool = GlobalAveragePooling1D()(gru)\n",
    "\n",
    "        if self.emotion_dim != 0:\n",
    "            emotion_input = Input(\n",
    "                shape=(self.emotion_dim,), name='emotion-input')\n",
    "            emotion_enhanced = Concatenate()([avg_pool, emotion_input])\n",
    "\n",
    "            dense = Dense(32, activation='relu', kernel_regularizer=l2(\n",
    "                self.l2_param))(emotion_enhanced)\n",
    "            output = Dense(self.category_num, activation='softmax',\n",
    "                           kernel_regularizer=l2(self.l2_param))(dense)\n",
    "            model = Model(\n",
    "                inputs=[semantic_input, emotion_input], outputs=output)\n",
    "        else:\n",
    "            dense = Dense(32, activation='relu',\n",
    "                          kernel_regularizer=l2(self.l2_param))(avg_pool)\n",
    "            output = Dense(self.category_num, activation='softmax',\n",
    "                           kernel_regularizer=l2(self.l2_param))(dense)\n",
    "            model = Model(inputs=[semantic_input], outputs=output)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           (None, 150, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           (None, 150, 150)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 150, 918)     0           input_40[0][0]                   \n",
      "                                                                 input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 150, 64)      182592      concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 65)           0           global_average_pooling1d_12[0][0]\n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           2112        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            33          dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 184,737\n",
      "Trainable params: 184,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sent_input = Input((150,768,), dtype='float32')\n",
    "pos_input = Input((150,150,), dtype='float32')\n",
    "concat = concatenate([sent_input, pos_input], axis = -1)\n",
    "gru = Bidirectional(GRU(32, return_sequences=True))(concat)\n",
    "avg_pool = GlobalAveragePooling1D()(gru)\n",
    "\n",
    "ss_input = Input((1,),dtype =\"float32\")\n",
    "emotion_enhanced = concatenate([avg_pool, ss_input], axis = -1)\n",
    "\n",
    "dense = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(emotion_enhanced)\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(dense)\n",
    "model = Model(inputs=[pos_input, sent_input, ss_input], outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "filepath=\"BIGRU.hdf5\"\n",
    "mcp_save = ModelCheckpoint(filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3377, 150, 768)\n",
      "(3377, 150, 150)\n",
      "(3377, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_metadata_train.shape)\n",
    "print(x_ss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_41 to have shape (150, 150) but got array with shape (150, 768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-1b91f1e11952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m hist = model.fit([x_train, x_metadata_train, x_ss], y_train, epochs=50, batch_size=32,\n\u001b[1;32m      3\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_metadata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ss_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcp_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                  )\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BIGRU.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haha/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haha/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/haha/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_41 to have shape (150, 150) but got array with shape (150, 768)"
     ]
    }
   ],
   "source": [
    "# test ans\n",
    "hist = model.fit([x_train, x_metadata_train, x_ss], y_train, epochs=50, batch_size=32,\n",
    "                validation_data=([x_val,  x_metadata_val, x_ss_val], y_val),\n",
    "                callbacks=[earlyStopping, mcp_save, reduce_lr_loss]\n",
    "                 )\n",
    "model.save(\"BIGRU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without multiple input acc: 0.85\n",
    "# with pos acc: 0.87\n",
    "model_path = \"BIGRU.h5\"\n",
    "a = load_model(model_path, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "x_test = test\n",
    "x_metadata = test_head_pos\n",
    "y_test = df.label\n",
    "\n",
    "loss, acc = a.evaluate([x_test, x_metadata, test_ss_pad] ,y_test, batch_size=64)\n",
    "print(\"Test set accuracy: \",acc)\n",
    "print(\"Test set loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of accuracy in each epoch\n",
    "accu_curve=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],\"r\")\n",
    "plt.plot(hist.history['val_accuracy'],\"b\")\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "accu_curve.savefig('accuracy_features.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot of loss in each epoch\n",
    "loss_curve = plt.figure()\n",
    "plt.plot(hist.history['loss'],\"r\")\n",
    "plt.plot(hist.history['val_loss'],\"b\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "loss_curve.savefig('loss_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# save the result in /preprocess/data  folder\n",
    "save_dir = './weibo20'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "def cut_words_from_text(text):\n",
    "    return list(jieba.cut(text))\n",
    "    \n",
    "    \n",
    "datasets_ch = [\"Weibo-20\"]\n",
    "\n",
    "for dataset in datasets_ch :\n",
    "    print('\\n\\n{} [{}]\\tProcessing the dataset: {} {}\\n'.format(\n",
    "        '-'*20, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()), dataset, '-'*20))\n",
    "\n",
    "    data_dir = os.path.join('/home/alissa77/WWW2021/dataset', dataset)\n",
    "    output_dir = os.path.join(save_dir, dataset)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    emotion_dir = os.path.join(output_dir, 'emotions')\n",
    "    if not os.path.exists(emotion_dir):\n",
    "        os.mkdir(emotion_dir)\n",
    "\n",
    "    split_datasets = [json.load(open(os.path.join(data_dir, '{}.json'.format(t)), 'r')) \n",
    "                      for t in ['train', 'val', 'test']]\n",
    "    split_datasets = dict(zip(['train', 'val', 'test'], split_datasets))\n",
    "\n",
    "    for t, pieces in split_datasets.items():\n",
    "        \n",
    "        # words cutting\n",
    "        for p in pieces:\n",
    "            p['content_words'] = cut_words_from_text(p['content'])\n",
    "            \n",
    "        with open(os.path.join(output_dir, '{}.json'.format(t)), 'w') as f:\n",
    "            json.dump(pieces, f, indent=4, ensure_ascii=False)\n",
    "        '''\n",
    "        emotion_arr = [extract_pkg.extract_dual_emotion(p) for p in tqdm(pieces)]\n",
    "        emotion_arr = np.array(emotion_arr)\n",
    "        print(emotion_arr)\n",
    "        print('{} dataset: got a {} emotion arr'.format(t, emotion_arr.shape))\n",
    "        np.save(os.path.join(emotion_dir, '{}_{}.npy'.format( t, emotion_arr.shape)), emotion_arr)\n",
    "        '''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('/home/alissa77/fake-news-detect/weibo20', dataset)\n",
    "split_datasets = [json.load(open(os.path.join(data_dir, '{}.json'.format(t)), 'r')) for t in ['train', 'val', 'test']]\n",
    "split_datasets = dict(zip(['train', 'val', 'test'], split_datasets))\n",
    "\n",
    "for t, pieces in split_datasets.items():\n",
    "    # words cutting\n",
    "    for p in pieces:\n",
    "        p[\"content_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "The num of negation words:  44\n",
      "The num of degree words:  214 . eg:  ('百分之百', 2.0)\n",
      "[Dalianligong]\tThere are 27351 words, the dimension is (29,)\n",
      "[Emoticon]\tThere are 104 emoticons, including 5 categories\n",
      "[HowNet]\tThere are 1899 positive words and 4315 negative words\n",
      "\n",
      "\n",
      "-------------------- [2021-08-13 17:28:55]\tProcessing the dataset: Weibo-16 --------------------\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'Requested level (！) does not match index name (None)'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-6ed15846324f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0memotion_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_dual_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-6ed15846324f>\u001b[0m in \u001b[0;36mextract_dual_emotion\u001b[0;34m(piece)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_dual_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     publisher_emotion = extract_publisher_emotion(\n\u001b[0;32m---> 28\u001b[0;31m         piece['content'], piece['content_words'], piece['content_emotions'])\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpublisher_emotion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6ed15846324f>\u001b[0m in \u001b[0;36mextract_publisher_emotion\u001b[0;34m(content, content_words, emotions_dict)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdalianligong_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboson_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauxilary_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6ed15846324f>\u001b[0m in \u001b[0;36mauxilary_features\u001b[0;34m(text, cut_words)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memoticon_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msymbols_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_words_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpronoun_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-6ed15846324f>\u001b[0m in \u001b[0;36msymbols_count\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;31m# Punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msymbols_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mexcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'！'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'？'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mcomma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'，'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m             \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0mlev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_level_number\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_level_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_index_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_validate_index_level\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             raise KeyError(\n\u001b[0;32m-> 1411\u001b[0;31m                 \u001b[0;34mf\"Requested level ({level}) does not match index name ({self.name})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m             )\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Requested level (！) does not match index name (None)'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "datasets_ch = ['Weibo-16','Weibo-20',]\n",
    "\n",
    "def cut_words_from_text(text):\n",
    "    return list(jieba.cut(text))\n",
    "def extract_publisher_emotion(content, content_words, emotions_dict):\n",
    "    text, cut_words = content, content_words\n",
    "    arr = np.zeros(55)\n",
    "\n",
    "    arr[8:37] = dalianligong_arr(cut_words)\n",
    "    arr[37:38] = boson_value(cut_words)\n",
    "    arr[38:55] = auxilary_features(text, cut_words)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def extract_dual_emotion(piece):\n",
    "    publisher_emotion = extract_publisher_emotion(\n",
    "        piece['content'], piece['content_words'], piece['content_emotions'])\n",
    "\n",
    "    return publisher_emotion\n",
    "def get_not_and_how_value(cut_words, i, windows):\n",
    "    not_cnt = 0\n",
    "    how_v = 1\n",
    "\n",
    "    left = 0 if (i - windows) < 0 else (i - windows)\n",
    "    window_text = ' '.join(cut_words[left:i])\n",
    "\n",
    "    for w in negation_words:\n",
    "        if w in window_text:\n",
    "            not_cnt += 1\n",
    "    for w in how_words_dict.keys():\n",
    "        if w in window_text:\n",
    "            how_v *= how_words_dict[w]\n",
    "\n",
    "    return (-1) ** not_cnt, how_v\n",
    "\n",
    "\n",
    "def boson_value(cut_words, windows=2):\n",
    "    value = 0\n",
    "    def b_edge(value):\n",
    "        if value<0:\n",
    "            return -4\n",
    "        else:\n",
    "            return 4\n",
    "    valence = {}\n",
    "    arousal = {}\n",
    "    cvaw_dict = dict()    \n",
    "    \n",
    "    with open('/home/alissa77/WWW2021 copy/code/emotion/cvaw4.csv', newline='') as csvfile:\n",
    "        rows = csv.DictReader(csvfile)\n",
    "        for row in rows:\n",
    "            valence[row['Word']] = (row['Valence_Mean'])\n",
    "            arousal[row['Word']] = (row['Arousal_Mean'])\n",
    "        #cvaw_dict[arousal[0]] = float(arousal[1])\n",
    "        \n",
    "    valence = {x: round(float(valence[x])-5, 2) for x in valence}\n",
    "    arousal = {x: round(float(arousal[x])-5, 2) for x in arousal}\n",
    "    #print('[CVAW]\\t There are {} words'.format(len(cvaw_dict)))\n",
    "\n",
    "    for i, word in enumerate(cut_words):\n",
    "        if word in arousal:\n",
    "            not_v, how_v = get_not_and_how_value(cut_words, i, windows)\n",
    "            \n",
    "            value += not_v * how_v * arousal[word]\n",
    "            \n",
    "    return value\n",
    "\n",
    "\n",
    "# ============================== Category ==============================\n",
    "baidu_emotions = ['angry', 'disgusting', 'fearful',\n",
    "                  'happy', 'sad', 'neutral', 'pessimistic', 'optimistic']\n",
    "baidu_emotions.sort()\n",
    "\n",
    "baidu_emotions_2_index = dict(\n",
    "    zip(baidu_emotions, [i for i in range(len(baidu_emotions))]))\n",
    "\n",
    "\n",
    "def baidu_arr(emotions_dict):\n",
    "    arr = np.zeros(len(baidu_emotions))\n",
    "\n",
    "    if emotions_dict is None:\n",
    "        return arr\n",
    "\n",
    "    for k, v in emotions_dict.items():\n",
    "        # like -> happy\n",
    "        if k == 'like':\n",
    "            arr[baidu_emotions_2_index['happy']] += v\n",
    "        else:\n",
    "            arr[baidu_emotions_2_index[k]] += v\n",
    "\n",
    "    return arr\n",
    "\n",
    "# ============================== Lexicon and Intensity ==============================\n",
    "# load negation words\n",
    "negation_words = []\n",
    "with open('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/others/negative/negationWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        negation_words.append(line.strip())\n",
    "\n",
    "print('\\nThe num of negation words: ', len(negation_words))\n",
    "\n",
    "# load degree words\n",
    "how_words_dict = dict()\n",
    "with open('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/HowNet/intensifierWords.txt', 'r') as src:\n",
    "    lines = src.readlines()\n",
    "    for line in lines:\n",
    "        how_word = line.strip().split()\n",
    "        how_words_dict[' '.join(how_word[:-1])] = float(how_word[-1])\n",
    "\n",
    "print('The num of degree words: ', len(how_words_dict),\n",
    "      '. eg: ', list(how_words_dict.items())[0])\n",
    "\n",
    "\n",
    "_, words2array = joblib.load(\n",
    "    '/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/大连理工大学情感词汇本体库/preprocess/words2array_27351.pkl')\n",
    "print('[Dalianligong]\\tThere are {} words, the dimension is {}'.format(\n",
    "    len(words2array), words2array['快乐'].shape))\n",
    "\n",
    "\n",
    "def dalianligong_arr(cut_words, windows=2):\n",
    "    arr = np.zeros(29)\n",
    "    \n",
    "    for i, word in enumerate(cut_words):\n",
    "        if word in words2array:\n",
    "            not_v, how_v = get_not_and_how_value(cut_words, i, windows)\n",
    "            arr += not_v * how_v * words2array[word]\n",
    "\n",
    "    return arr\n",
    "\n",
    "# ============================== Auxilary Features ==============================\n",
    "# Emoticon\n",
    "emoticon_df = pd.read_csv(\n",
    "    '/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/others/emoticon/emoticon.csv')\n",
    "emoticons = emoticon_df['emoticon'].tolist()\n",
    "emoticon_types = list(set(emoticon_df['label'].tolist()))\n",
    "emoticon_types.sort()\n",
    "emoticon2label = dict(\n",
    "    zip(emoticon_df['emoticon'].tolist(), emoticon_df['label'].tolist()))\n",
    "emoticon2index = dict(\n",
    "    zip(emoticon_types, [i for i in range(len(emoticon_types))]))\n",
    "\n",
    "print('[Emoticon]\\tThere are {} emoticons, including {} categories'.format(\n",
    "    len(emoticons), len(emoticon_types)))\n",
    "\n",
    "\n",
    "def emoticon_arr(text, cut_words):\n",
    "    arr = np.zeros(len(emoticon_types))\n",
    "\n",
    "    if len(cut_words) == 0:\n",
    "        return arr\n",
    "\n",
    "    for i, emoticon in enumerate(emoticons):\n",
    "        if emoticon in text:\n",
    "            arr[emoticon2index[emoticon2label[emoticon]]\n",
    "                ] += text.count(emoticon)\n",
    "\n",
    "    return arr / len(cut_words)\n",
    "\n",
    "\n",
    "# Punctuation\n",
    "def symbols_count(text):\n",
    "    excl = (text.count('！') + text.count('!')) / len(text)\n",
    "    ques = (text.count('？') + text.count('?')) / len(text)\n",
    "    comma = (text.count(',') + text.count('，')) / len(text)\n",
    "    dot = (text.count('.') + text.count('。')) / len(text)\n",
    "    ellip = (text.count('..') + text.count('。。')) / len(text)\n",
    "\n",
    "    return excl, ques, comma, dot, ellip\n",
    "\n",
    "\n",
    "# Sentimental Words??????? Hownet\n",
    "def init_words(file):\n",
    "    with open(file, 'r', encoding='utf-8') as src:\n",
    "        words = src.readlines()\n",
    "        words = [l.strip() for l in words]\n",
    "    return list(set(words))\n",
    "\n",
    "pos_words = init_words('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/HowNet/正面情感词语（中文）.txt')\n",
    "pos_words += init_words('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/HowNet/正面评价词语（中文）.txt')\n",
    "neg_words = init_words('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/HowNet/负面情感词语（中文）.txt')\n",
    "neg_words += init_words('/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/HowNet/负面评价词语（中文）.txt')\n",
    "pos_words = set(pos_words)\n",
    "neg_words = set(neg_words)\n",
    "print('[HowNet]\\tThere are {} positive words and {} negative words'.format(len(pos_words), len(neg_words)))\n",
    "\n",
    "# count sentiment_words\n",
    "def sentiment_words_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "    # positive and negative words\n",
    "    sentiment = []\n",
    "    for words in [pos_words, neg_words]:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            if word in cut_words:\n",
    "                c += 1\n",
    "        sentiment.append(c)\n",
    "    sentiment = [c / len(cut_words) for c in sentiment]\n",
    "\n",
    "    # degree words\n",
    "    degree = 0\n",
    "    for word in how_words_dict:\n",
    "        if word in cut_words:\n",
    "            degree += how_words_dict[word]\n",
    "\n",
    "    # negation words\n",
    "    negation = 0\n",
    "    for word in negation_words:\n",
    "        negation += cut_words.count(word)\n",
    "    negation /= len(cut_words)\n",
    "    sentiment += [degree, negation]\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "# Personal Pronoun\n",
    "first_pronoun = init_words(\n",
    "    '/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/others/pronoun/1-personal-pronoun.txt')\n",
    "second_pronoun = init_words(\n",
    "    '/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/others/pronoun/2-personal-pronoun.txt')\n",
    "third_pronoun = init_words(\n",
    "    '/home/alissa77/fake-news-detect/WWW2021/resources/Chinese/others/pronoun/3-personal-pronoun.txt')\n",
    "pronoun_words = [first_pronoun, second_pronoun, third_pronoun]\n",
    "\n",
    "\n",
    "def pronoun_count(cut_words):\n",
    "    if len(cut_words) == 0:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "    pronoun = []\n",
    "    for words in pronoun_words:\n",
    "        c = 0\n",
    "        for word in words:\n",
    "            c += cut_words.count(word)\n",
    "        pronoun.append(c)\n",
    "\n",
    "    return [c / len(cut_words) for c in pronoun]\n",
    "\n",
    "\n",
    "# Auxilary Features\n",
    "def auxilary_features(text, cut_words):\n",
    "    arr = np.zeros(17)\n",
    "    arr[:5] = emoticon_arr(text, cut_words)\n",
    "    arr[5:10] = symbols_count(text)\n",
    "    arr[10:14] = sentiment_words_count(cut_words)\n",
    "    arr[14:17] = pronoun_count(cut_words)\n",
    "\n",
    "    return arr\n",
    "\n",
    "for dataset in datasets_ch :\n",
    "    print('\\n\\n{} [{}]\\tProcessing the dataset: {} {}\\n'.format(\n",
    "        '-'*20, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()), dataset, '-'*20))\n",
    "\n",
    "    data_dir = os.path.join('/home/alissa77/fake-news-detect/preprocess_data', dataset)\n",
    "    output_dir = os.path.join(save_dir, dataset)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    emotion_dir = os.path.join(output_dir, 'emotions')\n",
    "    if not os.path.exists(emotion_dir):\n",
    "        os.mkdir(emotion_dir)\n",
    "\n",
    "    split_datasets = [pd.read_pickle(os.path.join(data_dir, '{}.pkl'.format(t))) for t in ['train', 'val', 'test']] \n",
    "    split_datasets = dict(zip(['train', 'val', 'test'], split_datasets))\n",
    "    \n",
    "    for t, p in split_datasets.items():\n",
    "        emotion_arr = extract_dual_emotion(p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/alissa77/WWW2021 copy/code/preprocess/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "haha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}